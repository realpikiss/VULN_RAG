# üñ•Ô∏è Guide GPU pour ComputeCanada Cedar

## V√©rification de la disponibilit√© des GPUs

### 1. Voir les partitions GPU
```bash
# Voir toutes les partitions disponibles
sinfo

# Voir sp√©cifiquement les partitions GPU
sinfo -p gpu
```

### 2. Voir les GPUs disponibles
```bash
# Voir les jobs GPU en cours
squeue -p gpu

# Voir les n≈ìuds GPU
sinfo -N -l | grep gpu
```

### 3. Types de GPUs sur Cedar
```bash
# V100 (16GB VRAM) - Plus commun
salloc --gres=gpu:v100:1

# A100 (40GB VRAM) - Plus puissant, plus rare
salloc --gres=gpu:a100:1

# RTX 6000 (24GB VRAM)
salloc --gres=gpu:rtx6000:1

# GPU g√©n√©rique (n'importe quel type)
salloc --gres=gpu:1
```

## Demander des ressources GPU

### 1. Session interactive avec GPU
```bash
# Demander un n≈ìud avec GPU V100
salloc --account=def-username --time=2:00:00 --mem=32G --cpus-per-task=8 --gres=gpu:v100:1

# Demander un n≈ìud avec GPU A100 (plus puissant)
salloc --account=def-username --time=2:00:00 --mem=64G --cpus-per-task=16 --gres=gpu:a100:1

# Demander plusieurs GPUs (si n√©cessaire)
salloc --account=def-username --time=4:00:00 --mem=64G --cpus-per-task=16 --gres=gpu:v100:2
```

### 2. Job batch avec GPU
```bash
# Soumettre un job avec GPU
sbatch --gres=gpu:v100:1 scripts/cedar/evaluation_gpu_job.sh

# Ou modifier le script pour inclure GPU
#SBATCH --gres=gpu:v100:1
```

## V√©rifier l'acc√®s GPU

### 1. Dans une session interactive
```bash
# V√©rifier les GPUs disponibles
nvidia-smi

# V√©rifier avec PyTorch
python -c "
import torch
print(f'CUDA disponible: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'Nombre de GPUs: {torch.cuda.device_count()}')
    print(f'GPU actuel: {torch.cuda.get_device_name()}')
    print(f'VRAM totale: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
else:
    print('Aucun GPU disponible')
"
```

### 2. Variables d'environnement GPU
```bash
# Voir les variables GPU
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "SLURM_CUDA_DEVICES: $SLURM_CUDA_DEVICES"

# Configurer pour utiliser le premier GPU
export CUDA_VISIBLE_DEVICES=0
```

## Optimisations GPU pour VulnRAG

### 1. Configuration PyTorch
```bash
# Optimisations m√©moire GPU
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Utiliser la pr√©cision mixte
export CUDA_LAUNCH_BLOCKING=1
```

### 2. Configuration Hugging Face
```python
# Dans votre code Python
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Configuration 4-bit pour √©conomiser la VRAM
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

# Charger le mod√®le avec quantification
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
)
```

### 3. Gestion de la m√©moire GPU
```python
# V√©rifier l'utilisation m√©moire
import torch
print(f"VRAM utilis√©e: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"VRAM r√©serv√©e: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

# Nettoyer la m√©moire
torch.cuda.empty_cache()
```

## Scripts SLURM avec GPU

### 1. Script d'√©valuation GPU
```bash
#!/bin/bash
#SBATCH --account=def-username
#SBATCH --time=2:00:00
#SBATCH --mem=32G
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:v100:1
#SBATCH --output=logs/evaluation_gpu_%j.out
#SBATCH --error=logs/evaluation_gpu_%j.err

module load python/3.9
module load cuda/11.4
module load java/11
source venv/bin/activate

# Configuration GPU
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Configuration Hugging Face
export HF_HOME=/scratch/$USER/vulnrag/huggingface
export TRANSFORMERS_CACHE=/scratch/$USER/vulnrag/huggingface/transformers

# Configuration Joern
export PATH="$HOME/.local/share/coursier/bin:$PATH"

# V√©rifier GPU
echo "GPU disponible: $CUDA_VISIBLE_DEVICES"
nvidia-smi

# Lancer l'√©valuation
python evaluation/detection/evaluation_runner.py \
  --detectors vulnrag-qwen2.5 vulnrag-kirito \
  --max-samples 50 \
  --use-gpu
```

### 2. Script de test GPU rapide
```bash
#!/bin/bash
#SBATCH --account=def-username
#SBATCH --time=0:30:00
#SBATCH --mem=16G
#SBATCH --cpus-per-task=4
#SBATCH --gres=gpu:v100:1
#SBATCH --output=logs/gpu_test_%j.out
#SBATCH --error=logs/gpu_test_%j.err

module load python/3.9
module load cuda/11.4
source venv/bin/activate

# Test GPU
echo "=== Test GPU ==="
nvidia-smi
python -c "
import torch
print(f'CUDA: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name()}')
    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
"

# Test mod√®le Hugging Face
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print('Test chargement mod√®le...')
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct', trust_remote_code=True)
print('‚úì Tokenizer charg√©')

model = AutoModelForCausalLM.from_pretrained(
    'Qwen/Qwen2.5-7B-Instruct',
    torch_dtype=torch.float16,
    device_map='auto',
    trust_remote_code=True
)
print('‚úì Mod√®le charg√© sur GPU')
"
```

## D√©pannage GPU

### 1. Probl√®mes courants

#### GPU non disponible
```bash
# V√©rifier si vous √™tes dans un job GPU
echo $SLURM_JOB_PARTITION
echo $CUDA_VISIBLE_DEVICES

# Si pas de GPU, demander des ressources
salloc --gres=gpu:v100:1
```

#### M√©moire GPU insuffisante
```bash
# V√©rifier l'utilisation m√©moire
nvidia-smi

# Utiliser la quantification 4-bit
# Voir configuration Hugging Face ci-dessus
```

#### Module CUDA non trouv√©
```bash
# Charger le module CUDA
module load cuda/11.4

# V√©rifier l'installation
nvcc --version
```

### 2. Logs utiles
```bash
# Logs SLURM
tail -f logs/evaluation_gpu_*.err

# Informations GPU
nvidia-smi -l 1  # Mise √† jour toutes les secondes

# Test PyTorch
python -c "import torch; print(torch.cuda.is_available())"
```

## Conseils pour l'utilisation des GPUs

### 1. Choisir le bon GPU
- **V100 (16GB)** : Bon pour la plupart des mod√®les 7B
- **A100 (40GB)** : Id√©al pour les mod√®les 14B+ et batch processing
- **RTX 6000 (24GB)** : Bon compromis

### 2. Optimiser l'utilisation
- Utiliser la quantification 4-bit pour √©conomiser la VRAM
- Nettoyer la m√©moire entre les jobs
- Utiliser `device_map="auto"` pour la gestion automatique

### 3. Monitoring
- Surveiller l'utilisation avec `nvidia-smi`
- V√©rifier les logs SLURM pour les erreurs
- Utiliser `torch.cuda.memory_summary()` pour le debugging

## Exemple complet d'utilisation

```bash
# 1. Demander des ressources GPU
salloc --account=def-username --time=2:00:00 --mem=32G --cpus-per-task=8 --gres=gpu:v100:1

# 2. V√©rifier l'acc√®s GPU
nvidia-smi
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# 3. Lancer l'√©valuation
sbatch scripts/cedar/evaluation_gpu_job.sh

# 4. Surveiller
squeue -u username
tail -f logs/evaluation_gpu_*.out
``` 